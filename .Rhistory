} else {
return(data.frame(location = location, lat = NA, lon = NA, stringsAsFactors = FALSE))
}
}, error = function(e) {
return(data.frame(location = location, lat = NA, lon = NA, stringsAsFactors = FALSE))
})
}, .id = "location")
gc()
gc()
# Loading necessary libraries for data manipulation, text analysis, geocoding, plotting, and more.
library(readr)
library(dplyr)
library(lubridate)
library(quanteda)
library(quanteda.textstats)
library(leaflet)
library(leaflet.extras2)
library(plotly)
library(tidyr)
library(maps)
library(ggmap)
library(purrr)
library(lubridate)
library(corrplot)
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
# Deleting any pre-existing processed files to ensure a clean start.
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_AnimeList.csv")
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_UserList.csv")
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_UserAnimeList.csv")
# Defining functions to process each dataset by filtering and selecting relevant columns.
process_anime_chunk <- function(df, pos) {
df %>%
filter(score > 6) %>%
select(anime_id, title, genre, episodes, score) %>%
write_csv("processed_AnimeList.csv", append = TRUE)
}
process_user_chunk <- function(df, pos) {
df %>%
filter(user_completed > 50) %>%
select(user_id, username, gender, birth_date) %>%
write_csv("processed_UserList.csv", append = TRUE)
}
process_user_anime_chunk <- function(df, pos) {
df %>%
filter(my_score > 0) %>%
select(username, anime_id, my_score, my_status) %>%
write_csv("processed_UserAnimeList.csv", append = TRUE)
}
# Setting a chunk size for processing large datasets efficiently.
chunk_size <- 20000
# Processing each dataset chunk by chunk and saving the cleaned data.
# Process and save cleaned AnimeList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/AnimeList.csv", DataFrameCallback$new(process_anime_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
# Process and save cleaned UserList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserList.csv", DataFrameCallback$new(process_user_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
# Process and save cleaned UserAnimeList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserAnimeList.csv", DataFrameCallback$new(process_user_anime_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
gc()
# Loading processed datasets and combining them into a single dataframe.
anime_df <- read_csv("processed_AnimeList.csv")
user_df <- read_csv("processed_UserList.csv")
user_anime_df <- read_csv("processed_UserAnimeList.csv")
# Adjust column names as necessary for all dataframes
colnames(user_anime_df) <- c("username", "anime_id", "my_watched_episodes", "my_score")
colnames(anime_df) <- c("anime_id", "title", "genre", "episodes", "score")
colnames(user_df) <- c("username", "user_id", "gender", "birth_date")
# Ensure 'username' columns are character types for joining
user_df <- user_df %>% mutate(username = as.character(username))
user_anime_df <- user_anime_df %>% mutate(username = as.character(username))
# Combine the dataframes
combined_df <- user_anime_df %>%
inner_join(anime_df, by = "anime_id") %>%
inner_join(user_df, by = "username")
combined_df <- combined_df %>%
mutate(user_age = 2023 - year(birth_date), gender = as.factor(gender))
# Conducting GLM analysis to explore relationships between user scores, age, and gender.
# GLM Analysis
glm_model <- glm(my_score ~ user_age + gender, data = combined_df, family = gaussian())
summary(glm_model)
# Text Analysis on Anime Titles
tokens_titles <- tokens(combined_df$title, remove_punct = TRUE) %>%
tokens_remove(pattern = stopwords("english"))
dfm_titles <- dfm(tokens_titles)
top_terms <- textstat_frequency(dfm_titles, n = 10)
print(top_terms)
# Alternative Method: Calculate Term Frequencies Directly
term_matrix <- as.matrix(dfm_titles)
term_sums <- colSums(term_matrix)
top_terms <- sort(term_sums, decreasing = TRUE)[1:10]
top_terms_df <- data.frame(term = names(top_terms), frequency = top_terms)
print(top_terms_df)
# Performing text analysis on anime titles to identify frequent terms and thematic trends.
# Load the dataset
user_list <- read_csv("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserList.csv")
head(user_list)
# Remove rows where location is blank and drop unnecessary columns.
user_list_cleaned <- user_list %>%
filter(!is.na(location) & location != "") %>%
select(username, location, gender)  # Keeping only necessary columns
# Since locations are in 'city, country' format, let's split them for easier manipulation
user_list_cleaned <- user_list_cleaned %>%
separate(location, into = c("city", "country"), sep = ", ", extra = "merge") %>%
mutate(country = trimws(country))  # Removing any leading/trailing whitespace
# A quick check to see the cleaned data
head(user_list_cleaned)
# Conducting geographical analysis by mapping user locations with local datasets and visualizing them using Leaflet.
# Load natural earth data for countries
countries_sf <- ne_countries(scale = "medium", returnclass = "sf")
# Load natural earth data for countries
countries_sf <- ne_countries(scale = "medium", returnclass = "sf")
# Load the dataset
user_list <- read_csv("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserList.csv")
head(user_list)
# Remove rows where location is blank and drop unnecessary columns
# Assuming the columns are named 'city' and 'country' for simplicity. Adjust as per your dataset structure.
user_list_cleaned <- user_list %>%
filter(!is.na(location) & location != "") %>%
select(username, location, gender)  # Keeping only necessary columns
# Since locations are in 'city, country' format, let's split them for easier manipulation
user_list_cleaned <- user_list_cleaned %>%
separate(location, into = c("city", "country"), sep = ", ", extra = "merge") %>%
mutate(country = trimws(country))  # Removing any leading/trailing whitespace
# A quick check to see the cleaned data
head(user_list_cleaned)
# Load natural earth data for countries
countries_sf <- ne_countries(scale = "medium", returnclass = "sf")
# Loading necessary libraries for data manipulation, text analysis, geocoding, plotting, and more.
library(readr)
library(dplyr)
library(lubridate)
library(quanteda)
library(quanteda.textstats)
library(leaflet)
library(leaflet.extras2)
library(plotly)
library(tidyr)
library(maps)
library(ggmap)
library(purrr)
library(lubridate)
library(corrplot)
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
library(rnaturalearth)
library(sf)
library(purrr)
# Deleting any pre-existing processed files to ensure a clean start.
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_AnimeList.csv")
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_UserList.csv")
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_UserAnimeList.csv")
# Defining functions to process each dataset by filtering and selecting relevant columns.
process_anime_chunk <- function(df, pos) {
df %>%
filter(score > 6) %>%
select(anime_id, title, genre, episodes, score) %>%
write_csv("processed_AnimeList.csv", append = TRUE)
}
process_user_chunk <- function(df, pos) {
df %>%
filter(user_completed > 50) %>%
select(user_id, username, gender, birth_date) %>%
write_csv("processed_UserList.csv", append = TRUE)
}
process_user_anime_chunk <- function(df, pos) {
df %>%
filter(my_score > 0) %>%
select(username, anime_id, my_score, my_status) %>%
write_csv("processed_UserAnimeList.csv", append = TRUE)
}
# Setting a chunk size for processing large datasets efficiently.
chunk_size <- 20000
# Loading processed datasets and combining them into a single dataframe.
anime_df <- read_csv("processed_AnimeList.csv")
# Defining functions to process each dataset by filtering and selecting relevant columns.
process_anime_chunk <- function(df, pos) {
df %>%
filter(score > 6) %>%
select(anime_id, title, genre, episodes, score) %>%
write_csv("processed_AnimeList.csv", append = TRUE)
}
process_user_chunk <- function(df, pos) {
df %>%
filter(user_completed > 50) %>%
select(user_id, username, gender, birth_date) %>%
write_csv("processed_UserList.csv", append = TRUE)
}
process_user_anime_chunk <- function(df, pos) {
df %>%
filter(my_score > 0) %>%
select(username, anime_id, my_score, my_status) %>%
write_csv("processed_UserAnimeList.csv", append = TRUE)
}
# Setting a chunk size for processing large datasets efficiently.
chunk_size <- 20000
# Setting a chunk size for processing large datasets efficiently.
chunk_size <- 20000
# Processing each dataset chunk by chunk and saving the cleaned data.
# Process and save cleaned AnimeList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/AnimeList.csv", DataFrameCallback$new(process_anime_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
# Process and save cleaned UserList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserList.csv", DataFrameCallback$new(process_user_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
# Process and save cleaned UserAnimeList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserAnimeList.csv", DataFrameCallback$new(process_user_anime_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
gc()
# Loading necessary libraries for data manipulation, text analysis, geocoding, plotting, and more.
library(readr)
library(dplyr)
library(lubridate)
library(quanteda)
library(quanteda.textstats)
library(leaflet)
library(leaflet.extras2)
library(plotly)
library(tidyr)
library(maps)
library(ggmap)
library(purrr)
library(lubridate)
library(corrplot)
library(reshape2)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
library(rnaturalearth)
library(sf)
library(purrr)
# Deleting any pre-existing processed files to ensure a clean start.
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_AnimeList.csv")
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_UserList.csv")
file.remove("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/processed_UserAnimeList.csv")
# Defining functions to process each dataset by filtering and selecting relevant columns.
process_anime_chunk <- function(df, pos) {
df %>%
filter(score > 6) %>%
select(anime_id, title, genre, episodes, score) %>%
write_csv("processed_AnimeList.csv", append = TRUE)
}
process_user_chunk <- function(df, pos) {
df %>%
filter(user_completed > 50) %>%
select(user_id, username, gender, birth_date) %>%
write_csv("processed_UserList.csv", append = TRUE)
}
process_user_anime_chunk <- function(df, pos) {
df %>%
filter(my_score > 0) %>%
select(username, anime_id, my_score, my_status) %>%
write_csv("processed_UserAnimeList.csv", append = TRUE)
}
# Setting a chunk size for processing large datasets efficiently.
chunk_size <- 20000
# Processing each dataset chunk by chunk and saving the cleaned data.
# Process and save cleaned AnimeList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/AnimeList.csv", DataFrameCallback$new(process_anime_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
# Process and save cleaned UserList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserList.csv", DataFrameCallback$new(process_user_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
# Process and save cleaned UserAnimeList data
read_csv_chunked("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserAnimeList.csv", DataFrameCallback$new(process_user_anime_chunk), chunk_size = chunk_size, locale = locale(encoding = "UTF-8"))
# Loading processed datasets and combining them into a single dataframe.
anime_df <- read_csv("processed_AnimeList.csv")
user_df <- read_csv("processed_UserList.csv")
user_anime_df <- read_csv("processed_UserAnimeList.csv")
# Adjust column names as necessary for all dataframes
colnames(user_anime_df) <- c("username", "anime_id", "my_watched_episodes", "my_score")
colnames(anime_df) <- c("anime_id", "title", "genre", "episodes", "score")
colnames(user_df) <- c("username", "user_id", "gender", "birth_date")
# Ensure 'username' columns are character types for joining
user_df <- user_df %>% mutate(username = as.character(username))
user_anime_df <- user_anime_df %>% mutate(username = as.character(username))
# Combine the dataframes
combined_df <- user_anime_df %>%
inner_join(anime_df, by = "anime_id") %>%
inner_join(user_df, by = "username")
combined_df <- combined_df %>%
mutate(user_age = 2023 - year(birth_date), gender = as.factor(gender))
# Conducting GLM analysis to explore relationships between user scores, age, and gender.
# GLM Analysis
glm_model <- glm(my_score ~ user_age + gender, data = combined_df, family = gaussian())
summary(glm_model)
# Text Analysis on Anime Titles
tokens_titles <- tokens(combined_df$title, remove_punct = TRUE) %>%
tokens_remove(pattern = stopwords("english"))
dfm_titles <- dfm(tokens_titles)
top_terms <- textstat_frequency(dfm_titles, n = 10)
print(top_terms)
# Alternative Method: Calculate Term Frequencies Directly
term_matrix <- as.matrix(dfm_titles)
term_sums <- colSums(term_matrix)
top_terms <- sort(term_sums, decreasing = TRUE)[1:10]
top_terms_df <- data.frame(term = names(top_terms), frequency = top_terms)
print(top_terms_df)
# Performing text analysis on anime titles to identify frequent terms and thematic trends.
# Load the dataset
user_list <- read_csv("C:/Users/vikra/OneDrive/Desktop/NUIG/SEM2/R-Prog/Ass3/DataSet/UserList.csv")
head(user_list)
# Remove rows where location is blank and drop unnecessary columns.
user_list_cleaned <- user_list %>%
filter(!is.na(location) & location != "") %>%
select(username, location, gender)  # Keeping only necessary columns
# Since locations are in 'city, country' format, let's split them for easier manipulation
user_list_cleaned <- user_list_cleaned %>%
separate(location, into = c("city", "country"), sep = ", ", extra = "merge") %>%
mutate(country = trimws(country))  # Removing any leading/trailing whitespace
# A quick check to see the cleaned data
head(user_list_cleaned)
# Conducting geographical analysis by mapping user locations with local datasets and visualizing them using Leaflet.
countries <- unique(user_list_cleaned$country)
# Set your API key for a geocoding service
register_google(key = "AIzaSyAonoPw1FAZXAUGMtnfi_L3I0QZFIq-_xk")
# Function to geocode a location
geocode_location <- function(location_name) {
geocode(location_name, output = "latlona", source = "google")
}
# Geocoding a single location as an example
geocoded_location <- geocode_location("Tokyo, Japan")
print(geocoded_location)
unique_locations <- unique(user_list_cleaned$country)
geocode_location <- function(location_name) {
if (is.na(location_name) || location_name == "") {
return(data.frame(lat = NA, lon = NA, stringsAsFactors = FALSE))
} else {
return(geocode(location_name, output = "latlona", source = "google"))
}
}
# Ensure there are no NAs or empty strings in unique_locations
unique_locations <- na.omit(unique_locations)  # Remove NA values
unique_locations <- unique_locations[unique_locations != ""]  # Remove empty strings
# Re-initialize geocoded_results to ensure it's empty before the loop
geocoded_results <- data.frame(location = character(), lat = numeric(), lon = numeric(), stringsAsFactors = FALSE)
# Use map_df to apply geocode_location to each location and combine the results
geocoded_results <- map_df(unique_locations, function(location) {
Sys.sleep(0.2)  # Slight pause to respect API rate limits; adjust based on your API's requirements
tryCatch({
geocoded <- geocode_location(location)
if (!is.na(geocoded$lat)) {
return(data.frame(location = location, lat = geocoded$lat, lon = geocoded$lon, stringsAsFactors = FALSE))
} else {
return(data.frame(location = location, lat = NA, lon = NA, stringsAsFactors = FALSE))
}
}, error = function(e) {
return(data.frame(location = location, lat = NA, lon = NA, stringsAsFactors = FALSE))
})
}, .id = "location")
# Conducting geographical analysis by mapping user locations with local datasets and visualizing them using Leaflet.
countries <- unique(user_list_cleaned$country)
# Set your API key for a geocoding service
register_google(key = "AIzaSyBVOOWj_myHzxciccZTJkLaEX96toEO4rk")
# Function to geocode a location
geocode_location <- function(location_name) {
geocode(location_name, output = "latlona", source = "google")
}
# Geocoding a single location as an example
geocoded_location <- geocode_location("Tokyo, Japan")
print(geocoded_location)
unique_locations <- unique(user_list_cleaned$country)
geocode_location <- function(location_name) {
if (is.na(location_name) || location_name == "") {
return(data.frame(lat = NA, lon = NA, stringsAsFactors = FALSE))
} else {
return(geocode(location_name, output = "latlona", source = "google"))
}
}
# Ensure there are no NAs or empty strings in unique_locations
unique_locations <- na.omit(unique_locations)  # Remove NA values
unique_locations <- unique_locations[unique_locations != ""]  # Remove empty strings
# Re-initialize geocoded_results to ensure it's empty before the loop
geocoded_results <- data.frame(location = character(), lat = numeric(), lon = numeric(), stringsAsFactors = FALSE)
# Use map_df to apply geocode_location to each location and combine the results
geocoded_results <- map_df(unique_locations, function(location) {
Sys.sleep(0.2)  # Slight pause to respect API rate limits; adjust based on your API's requirements
tryCatch({
geocoded <- geocode_location(location)
if (!is.na(geocoded$lat)) {
return(data.frame(location = location, lat = geocoded$lat, lon = geocoded$lon, stringsAsFactors = FALSE))
} else {
return(data.frame(location = location, lat = NA, lon = NA, stringsAsFactors = FALSE))
}
}, error = function(e) {
return(data.frame(location = location, lat = NA, lon = NA, stringsAsFactors = FALSE))
})
}, .id = "location")
leaflet(data = geocoded_results) %>%
addTiles() %>%
addMarkers(lng = ~lon, lat = ~lat, popup = ~location, clusterOptions = markerClusterOptions())
# Analyzing gender distribution among users and visualizing the results.
gender_distribution <- user_list_cleaned %>%
group_by(country, gender) %>%
summarise(Count = n(), .groups = 'drop')
ggplot(user_list_cleaned, aes(x = gender)) +
geom_bar(fill = "cornflowerblue") +
theme_minimal() +
labs(title = "Gender Distribution of Users", x = "Gender", y = "Count")
# Calculating user ages from birth dates and analyzing the age distribution.
user_df <- user_df %>%
mutate(
birth_date = as.Date(birth_date),
age = as.integer(difftime(Sys.Date(), birth_date, units = "days") / 365.25)  # Calculate age
)
summary(user_df$age)  # Quick summary to check age values
head(user_df$age, n = 20)  # Preview first few age values
str(user_df$age)  # Check the structure and data type of the age column
user_df <- user_df %>%
mutate(age = as.integer(format(Sys.Date(), "%Y")) - as.integer(format(birth_date, "%Y")))
# Adjust for birthdays that have not yet occurred this year
user_df <- user_df %>%
mutate(age = if_else(month(birth_date) > month(Sys.Date()) |
(month(birth_date) == month(Sys.Date()) & day(birth_date) > day(Sys.Date())),
age - 1, age))
ggplot(user_df, aes(x = age)) +
geom_histogram(binwidth = 3, fill = "skyblue", color = "black") +
theme_minimal() +
labs(title = "Age Distribution of Users", x = "Age", y = "Count") +
xlim(10, 70)
age_summary <- user_df %>%
summarise(
Mean_Age = mean(age, na.rm = TRUE),
Median_Age = median(age, na.rm = TRUE),
Min_Age = min(age, na.rm = TRUE),
Max_Age = max(age, na.rm = TRUE),
Q1 = quantile(age, 0.25, na.rm = TRUE),
Q3 = quantile(age, 0.75, na.rm = TRUE)
)
print(age_summary)
# Histogram to show age distribution by gender
ggplot(user_df, aes(x = age, fill = gender)) +
geom_histogram(position = "dodge", binwidth = 20) +  # Changed binwidth to 5 for clarity
theme_minimal() +
labs(title = "Age Distribution by Gender", x = "Age", y = "Count") +
xlim(10, 70)
age_summary <- user_df %>%
summarise(
Mean_Age = mean(age, na.rm = TRUE),
Median_Age = median(age, na.rm = TRUE),
Min_Age = min(age, na.rm = TRUE),
Max_Age = max(age, na.rm = TRUE),
Q1 = quantile(age, 0.25, na.rm = TRUE),
Q3 = quantile(age, 0.75, na.rm = TRUE)
)
print(age_summary)
# Histogram to show age distribution by gender
ggplot(user_df, aes(x = age, fill = gender)) +
geom_histogram(position = "dodge", binwidth = 20) +  # Changed binwidth to 5 for clarity
theme_minimal() +
labs(title = "Age Distribution by Gender", x = "Age", y = "Count") +
xlim(10, 70)
# Remove rows where 'user_age' or 'my_score' is NA, NaN, or Inf
cleaned_df <- combined_df %>%
filter(!is.na(user_age) & !is.na(my_score) &
!is.nan(user_age) & !is.nan(my_score) &
user_age != Inf & user_age != -Inf &
my_score != Inf & my_score != -Inf)
# Impute missing 'user_age' and 'my_score' with their means
combined_df <- combined_df %>%
mutate(user_age = ifelse(is.na(user_age) | is.nan(user_age) | user_age == Inf | user_age == -Inf, mean(user_age, na.rm = TRUE), user_age),
my_score = ifelse(is.na(my_score) | is.nan(my_score) | my_score == Inf | my_score == -Inf, mean(my_score, na.rm = TRUE), my_score))
# Using the cleaned data for kmeans
clusters <- kmeans(cleaned_df[, c("user_age", "my_score")], centers = 3)
cleaned_df$cluster <- as.factor(clusters$cluster)
ggplot(cleaned_df, aes(x = user_age, y = my_score, color = cluster)) +
geom_point() +
labs(title = "Cluster Plot of Users by Age and Score", x = "User Age", y = "My Score")
cor_matrix <- cor(combined_df[, sapply(combined_df, is.numeric)], use = "complete.obs")
# Create heatmap using plotly
plot_ly(
x = colnames(cor_matrix),
y = colnames(cor_matrix),
z = cor_matrix,
type = "heatmap",
colors = colorRamp(c("blue", "white", "red"))
)
# Remove tokens that appear in less than 3 documents
dfm_filtered <- dfm_trim(dfm_titles, min_docfreq = 3)
# Now calculate the frequency of the remaining tokens
top_tokens_filtered <- textstat_frequency(dfm_filtered, n = 10)
print(top_tokens_filtered)
# Visualization of Top Word Frequencies
ggplot(top_tokens_filtered, aes(x = reorder(feature, frequency), y = frequency)) +
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top Word Frequencies in Anime Titles", x = "Words", y = "Frequency") +
theme_minimal()
# Now calculate the frequency of the remaining tokens
top_tokens_filtered <- textstat_frequency(dfm_filtered, n = 100)
print(top_tokens_filtered)
# Set the random seed for reproducibility
set.seed(3000)
wordcloud(words = top_tokens_filtered$feature,
freq = top_tokens_filtered$frequency * 3,  # Scale frequencies to make words larger
scale = c(3, 0.10),  # Increase the range of possible sizes
min.freq = 1,
max.words = 100,  # Set max number of words
random.order = FALSE,
rot.per = 0.10,
colors = brewer.pal(8, "Dark2"),
vfont = c("sans serif", "bold"))
print("Hello")
data <- 1:10
data
data = read.csv(file.choose[])
data = read.csv(file.choose())
# 1. Set Working Directory
# Create a new folder on your computer "AI_Omics_Internship_2025".
getwd()
getwd() # Getting the current directory
# 1. Set Working Directory
# Create a new folder on your computer "AI_Omics_Internship_2025".
setwd("C:\\AI_Omics_Internship_2025")
